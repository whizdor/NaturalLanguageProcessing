{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMs\n",
    "\n",
    "These are advanced vevrsions of GRUs (Gated Recurrent Units), which is a fomr of RNNs. \n",
    "\n",
    "## Adavantages of RNNs\n",
    "\n",
    "- Capture dependiecnies wthing a short range\n",
    "- Takes up less RAM, than other n-grma models\n",
    "\n",
    "## Disadvantages of RNNs\n",
    "\n",
    "- Struggles to capture long term dependencies\n",
    "- They are prone to vainshing and exploding gradinents\n",
    "\n",
    "### What are vanishing and exploding gradients?\n",
    "\n",
    "The gradients in RNN are calcualted using backpropagation through time. We know that the weight and baises for both the new word and the propagateed vector are the same through out the RNN. The gradient is basically proprotional to the sum of the partial derivatives of the products.\n",
    "\n",
    "Length of the partial derevative product denotes the distance that k is from t. THe entire term is the contribution of the hidden state k.\n",
    "\n",
    "- To deal with vanishing gradients, initializ the weights to the identity matrix and using ReLU activations. What this does is that it copies the values from the pervious state and the applies ReLU on it.\n",
    "- Gradient Clipping\n",
    "- Skip Connections\n",
    "\n",
    "LSTMs were designed speficially to mitigate this issue.\n",
    "\n",
    "## What are LSTMs\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
