{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams\n",
    "\n",
    "N-Grams allow us to calculate probabilitties of certain words occouring in a specific sequence. This can be used to build a\n",
    "- Auto-Complete Model\n",
    "- Search Sugesstion Tool\n",
    "\n",
    "An N-gram is a squence of N words (punctuation is treated as words)\n",
    "\n",
    "Corpus : I am happy because I am learning\n",
    "Unigram : {I, am, happy, because, learning}\n",
    "Bigram : {I am, am happy, happy because, ...}\n",
    "Trigrams : {Similarly Tripletes ...}\n",
    "\n",
    "### Unigram Probability\n",
    "Sort of obvious, no?\n",
    "\n",
    "### Bigram and N-Gram Probability\n",
    "\n",
    "P(am|I) = C(I am)/C(I)\n",
    "\n",
    "Similarly, a tri-gram probability\n",
    "\n",
    "P(w3 | w1^2) = C(w1^2w3)/C(w1^2)\n",
    "\n",
    "### How to model whole sentence using N-Gram Probability\n",
    "\n",
    "Use Chain Rule, \n",
    "P(A,B,C,D) = P(A) P(B|A) P(C|A,B) P(D|A,B,C)\n",
    "Now, to calcualte the probability each of these elements must have a pre assigned probability, which is very diffculat to assertain, as the number of words increases.\n",
    "\n",
    "So, in order to ease this, we simply use an approximation\n",
    "Markoff Assumption : The probability of a word depends only on some of its predecessors, and not all.\n",
    "\n",
    "thus,\n",
    "P(A,B,C,D) = P(A) P(B|A) P(C|B) P(D|B)\n",
    "\n",
    "### How to get this N-Gram Probability using a corpus?\n",
    "\n",
    "We use two new symbols to denote, (start  angle_brackets(s) and the end of sentences angle_brackets(/s)).\n",
    "\n",
    "add N-1 start tokens, for an N-Gram.\n",
    "\n",
    "Why do we need the end-token?\n",
    "\n",
    "Eg: \n",
    "angle_brackets(s) yes no\n",
    "angle_brackets(s) yes yes\n",
    "angle_brackets(s) no no\n",
    "\n",
    "Consider this corpus.\n",
    "\n",
    "P(\"angle_brackets(s)\" yes yes) = P(yes | angle_brackets(s)) P(yes | yes) = 2/3 * 1/3\n",
    "P(angle_brackets(s) yes no) = 1/3\n",
    "P(angle_brackets(s) no no) = 1/3\n",
    "P(angle_brackets(s) no yes) = 0\n",
    "\n",
    "Sum of this is 1\n",
    "\n",
    "Considier sentences of length 3,\n",
    "P(angle_brackets(s) yes yes yes) = ...\n",
    "\n",
    "Sum of this is also 1.\n",
    "\n",
    "\n",
    "What do we want? We want the sum of probabilties of all, two, three ... lenght sentences to be equal to one (taken all at once, not individually). Thus, we add end tokens.\n",
    "\n",
    "\n",
    "## How to populate and create a count matrix\n",
    "This matrix tells, how many times a word is followed by another word.\n",
    "Then we transform the count matrix to a probability matrix.\n",
    "\n",
    "# Perplexity\n",
    "\n",
    "This is a metric used to evaluate a language model. [80 10 10][Train Validation Test].\n",
    "\n",
    "### How to split the data?\n",
    "- Continous Text\n",
    "- Short Sequence of words, such as sentences.\n",
    "\n",
    "Perplexity is the measure of the complexity in a sample of text. It is used to tell us, if a set of sentences was written by humans, rather than by a simple program. [A text written by a human will have less perxplexity] [Closely realte to entropy]\n",
    "\n",
    "\n",
    "# Out Of Vocabulary Word\n",
    "These are words, that are outside the vocabulary (set of fixed questions, that are used for the alngauge model)\n",
    "\n",
    "Unknown Words are known as out of vocabulary words.\n",
    "Replace these words by \\<UNK\\>\n",
    "\n",
    "- Words, that become \n",
    "\n",
    "# Smoothing\n",
    "\n",
    "- It is not necessary, that every combinarion of two words will appear in the bigram (similarly N-gram). To solve this we use N-grams.\n",
    "\n",
    "add, one occourance to each. (instead, add k for a larger corpus)\n",
    "\n",
    "- Another approach, is Backoff i.e, using lower level n-grams. (eg. Katz backoff)\n",
    "- Stupid Backoff is another methods\n",
    "\n",
    "\n",
    "- Interpolation, can also be used.  [Interpolation constants can be learned from the validation test.]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
