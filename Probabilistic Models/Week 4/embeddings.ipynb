{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors/ Word Embeddings\n",
    "\n",
    "#### One Hot Vectors\n",
    "Suppose that you have a 1000 words vocabulary. Each word can be represented by a vector of dimention 1000 (with each vector, just representing that a particular word is that word or not (i.e made up of all 0s except ONE 1))\n",
    "- The size is huge\n",
    "- There is no embedded meaning\n",
    "\n",
    "#### Word Embedding Vectors\n",
    "Vectors based upon ideas, and have a relatively smaller size. Thse vectors carry the meaning of the words, and are comparitively more useful in NLP Applications.\n",
    "\n",
    "## How do we create Word Embeddings from a Corpus?\n",
    "\n",
    "For training word-embeddings, a semi-supervised approach is use, as whilst the trainging data set is not labelled, it does conta data to supervise this learning proces. This combination of supervised and undupervised learning is known as self-supervised learning.\n",
    "\n",
    "## Word Embedding Methods\n",
    "1. word2vec (Google) : This uses a shallow neural netword.\n",
    "- continous-bag-of-words\n",
    "- continous skip-gram\n",
    "\n",
    "2. Global Vectors (GloVe) [Stanford] : does not use neural networs at all.\n",
    "\n",
    "3. fastText [Facebook]\n",
    "- this model supports, out of vocabulary words, by inferring their meaning from the sequence of characters used in the word.\n",
    "- word embedding vectors can be averaged together to create meaning of the sentences.\n",
    "\n",
    "4. Deep Learning, Contextual Embaddings [Transformer Methods.]\n",
    "- BERT [Google]\n",
    "- ELMo [Allen Institute of AI]\n",
    "- GPT-2 [OpenAI]\n",
    "\n",
    "# Continous-Bag-Of-Words Model\n",
    "\n",
    "If two words are frequently surrounded by each other, it is highly likely that they are related in meaning (i.e semantically related.) \n",
    "The context words (C, which is a hyperparameter), is fed in, and the centre word is attempted to be predicted.\n",
    "\n",
    "### Cleaning\n",
    "- Words should be case insensitive\n",
    "- Puntuations, can be represented as a single unique words. Non-Inturrepting Puntucations can be ignored\n",
    "- Numbers, can be ignored, but some numbers can have significance.\n",
    "- Special Characters: It is usually sage to drop them\n",
    "- Special Words : Emoji, or HashTags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# form nltk.tokenize import word_tokenize\n",
    "# import emoji\n",
    "\n",
    "# nltk.download('punkt') # punck is a pre-trained tokenizer for English, which handles punctuations.\n",
    "\n",
    "# courpus = \"Who ❤️ \"word embeddings\" in 2022? I do!!!\"\n",
    "\n",
    "# data = re.sub(r'[,!?;-]+' , '.', corpus)\n",
    "# #corpus now becomes: Who ❤️ \"word embeddings\" in 2022. I do.\n",
    "\n",
    "# data = nltk.word_tokenize(data)\n",
    "# data = [ch.lower for ch in data if ch.isalpha() or ch == '.' or emoji.get_emoji_regexp().search(ch)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_windows(words, C):\n",
    "#     i = C\n",
    "#     while(i < len(words) - C):\n",
    "#         center_word = words[i]\n",
    "#         context_words = word[(i-C):i] + words[(i+1):(i+C+1)]\n",
    "#         yeild context_words, center_word  #yeild does not immediately exit a function\n",
    "#         i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Architecture of the CBOW Model\n",
    "\n",
    "It has one input layer, one hidden layer and one output layer.\n",
    "\n",
    "### Dimensions\n",
    "X = Context words matrix is a V*m matrix\n",
    "H = is a N*m matrix\n",
    "Y(hat) = is a V*m matrix\n",
    "m is the batch size.\n",
    "\n",
    "### Activation Function\n",
    "ReLU \n",
    "and softmax\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "Loss Function used here is the cross-entropy loss function which is often used alogn with the soft max function\n",
    "\n",
    "J = - sigma (y_k log(y_k^))\n",
    "\n",
    "## Forward and Back Propagation\n",
    "\n",
    "Back Propagation calculats the partial derivatives of the cost with respect to weights and biases. This is a prime example of dynamic programming.\n",
    "\n",
    "J_batch = f(W1, W2, b1, b2)\n",
    "\n",
    "The other concept is using, Gradient Descent, after having the gradients, using the learning parameter (alpha).\n",
    "A smaller alpha allows for a slower update of the weights and biases.\n",
    "\n",
    "## Extracting the Word Embeddings, from the weights predicted by the NeuralNet\n",
    "\n",
    "1. Take W1\n",
    "2. Take W2\n",
    "3. Take an average for W1 and W2\n",
    "\n",
    "\n",
    "# Evaluation Metrics\n",
    "\n",
    "1. Intrinsic Evaluation\n",
    "It tests the relationships between the words. [Both semantic and syntax]. \n",
    "- This can be done by finding missing analogies. There is a possibility of ambuigity.\n",
    "- Thic can also be done by ysing Clustering.\n",
    "- Visualization\n",
    "\n",
    "2. Extrincsic Evaluation\n",
    "This is used to evaluate for external tasks. These are the ultimate tests to ensure that a model does what it was intended to do.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
